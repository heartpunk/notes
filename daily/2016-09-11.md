- [Brainpickings post about why we write](https://www.brainpickings.org/2016/09/07/jennifer-egan-on-writing/), as well as writing for others' approval. Good thoughts in here, and glad I read it. Two things from it I liked and want to explore more (they link to other brainpickings posts):
  - [“human beings are works in progress [who] mistakenly think they’re finished”](https://www.brainpickings.org/2014/06/18/daniel-gilbert-happiness-future-self/).
  - Learned about the concept of [deep play](https://www.brainpickings.org/2016/08/04/diane-ackerman-deep-play/), a concept apparently originating with Diane Ackerman.
- Farnam Street on [how people who are often write change their mind a lot](https://www.farnamstreetblog.com/2012/10/some-advice-from-jeff-bezos/). I liked this, a number of good quotes about changing one's mind.
- A nice addition to [Nassim Taleb's advice on productivity](https://twitter.com/nntaleb/status/753198628093698049) that I came up with. I had some other nice thoughts about this in today and yesterday's messenger stuff with Veronica, but the essence of it is here, as well as in the idea that pursuing your interest is a safe strategy.
  1. If you get stuck, figure out why, and address that shortcoming. Don't try to power through the thing you're stuck on.
  2. If you get stuck, or have to take a large detour from one topic to another, take a note about the connections.
- Learned what a [polynya](https://en.wikipedia.org/wiki/Polynya) is, and that's cool.
- Great fucking Victor Hugo quote from Les Miserables:
  - “When the nettle is young, the leaves make excellent greens; when it grows old it has filaments and fibers like hemp and flax. Cloth made from the nettle is as good as that made from hemp. Chopped up, the nettle is good for poultry; pounded, it is good for horned cattle. The seed of the nettle mixed with the fodder of animals gives a luster to their skin; the root, mixed with salt, produces a beautiful yellow dye. It makes, however, excellent hay, as it can be cut twice in a season. And what does the nettle need? very little soil, no care, no culture; except that the seeds fall as fast as they ripen, and it is difficult to gather them; that is all. If we would take a little pains, the nettle would be useful; we neglect it, and it becomes harmful. Then we kill it. How much men are like the nettle! My friends, remember this, that there are no weeds, and no worthless men, there are only bad farmers.”
- Interesting [summary](https://www.technologyreview.com/s/602344/the-extraordinary-link-between-deep-neural-networks-and-the-nature-of-the-universe/) of a [paper](http://arxiv.org/abs/1608.08225) about why deep learning is so effective at modeling our world.
- [Why productivity is the wrong goal](https://williamjbowman.com/blog/2016/09/05/obligatory-blog-post-of-productivity-advice/).
- [Projectile is a project-level automation library for emacs](https://github.com/bbatsov/projectile).
- Amazing [visualization of the evolution of antibiotic resistance](http://www.theatlantic.com/science/archive/2016/09/stunning-videos-of-evolution-in-action/499136/), found via [this toot](https://twitter.com/edyong209/status/773944842296721408)
- [Tiny lisp computer](http://www.technoblogy.com/show?1GX1) is not quite a lisp machine, but it sure is cool!
- [Xi editor](https://github.com/google/xi-editor) is a from the ground up new take on editing, seemingly mostly aimed at high performance. Same guy [wrote about the fastest font renderer in the world](https://medium.com/@raphlinus/inside-the-fastest-font-renderer-in-the-world-75ae5270c445#.n02ieidqi), which it seems he wrote.
- From the last item, discovered that [rust style iterators are on their way to C++](https://github.com/ericniebler/range-v3). Not sure if this is for real, but it's cool!
- [School of life video on self compassion is badass](https://www.youtube.com/watch?v=-kfUE41-JFw).
- [Series of tweets](https://twitter.com/mrdrozdov/status/775056290968109057) recommended [the Deep Learning Book](http://www.deeplearningbook.org/) and [Probabilistic Graphical Models](https://www.amazon.com/Probabilistic-Graphical-Models-Principles-Computation/dp/0262013193/ref=sr_1_1?ie=UTF8&qid=1473639219&sr=8-1&keywords=probabilistic+graphical+models) by Daphne Koller instead of Murphy's [Machine Learning: A Probabilistic Perspective](https://mitpress.mit.edu/books/machine-learning-0). I don't intend to do any of these soon, but it's good to know about.
- Watched [a Nassim Taleb explanation of fragility](https://www.youtube.com/watch?v=UgS2feyXmWw), had one really interesting point in it: fragile stress response curves are much less sensitive to very low amounts of stress than linear stress response, whereas much more sensitive to large amounts of stress. This is obvious when you think about it, but explains why fragility is sort of necessary. Also watched [another one](https://www.youtube.com/watch?v=WMS8ydyeb4E), but didn't get much out of it.
- [Apache Kafka and the Next 700 Stream Processing Systems](https://www.youtube.com/watch?v=9RMOc0SwRro) had some cool ways of thinking about architecture.
  - Some ideas about how to characterize the time based aspects of architecture:
    - Request/response systems have low latency and low throughput (generally processing one input at a time).
    - Batch systems have high latency and incredibly high throughput. Often run once a day or less frequently.
    - Stream systems mix the two, or make the distinction somewhat tunable. A stream system receives the data as it comes in, and can produce new data as it chooses. It can also amend values later, to allow for low latency output.
  - The metaphor of pulling the database log out of the database is back again, here. They really do love it, and rightfully so.
- [Strange Loops: Capturing Knots with Powerful Notations](https://www.youtube.com/watch?v=Wahc9Ocka1g) talks about notation through the lens of knot theory. One highlight is that notation varies on a few dimensions. While this is explicitly about notation, it probably applies far more generally. Notation *is* math, in some sense, and so consequently this is sort of about the tradeoffs in approaches to mathematical models themselves. I don't think the presenter would make quite this bold a claim, but it's what it makes me think.
  - How easy it for different consumers to use?
    - Computers
    - Humans
  - How easy it to use for different use cases? These can vary by kind of consumer
    - Reading
    - Writing
    - Manipulation
- [All In With Determinism for Performance and Testing in Distributed Systems](https://www.youtube.com/watch?v=gJRj3vJL4wE) had some great ideas about testing databases, and reminded me of just how cool VoltDB is. Glad I watched this. They take a really cool approach.
  - Full serializability is the only option in VoltDB.
  - Coordination node orders writes, then they're sent off to replicas.
  - Replicas hash inputs to writes, send hashes back to coordinating node. If hashes don't match, cluster shuts down.
  - Full DML is deterministic, reads are only deterministic if they're in a transaction with a write (this is known in advance since they only use stored procedures, I assume).
  - Can use the hashing technique to build a really cool self checking system. Do a bunch of reads and writes, hash at various stages, check that nothing is different between replicas, check that things that should be consecutive are, etc. They've found multiple bugs this way.
  - Decided not to do deterministic simulation as FoundationDB did, because it doesn't catch any bugs outside of the core state machine. OTOH, to be fair to FoundationDB: they did a similar kind of testing on top of their deterministic simulation, too.
- Really cool post covering how [exponential progress in computing is not limited to Moore's law](https://ourworldindata.org/technological-progress/).
